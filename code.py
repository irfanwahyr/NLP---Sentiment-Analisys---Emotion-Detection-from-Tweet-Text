# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUvfC0Y4bdn5h28XrNXMmeL8d8ft9cQZ

# DETEKSI EMOSI DARI TEXT MENGGUNAKAN LSTM

# Dataset merupakan data publik, diambil dari [link](https://www.kaggle.com/datasets/parulpandey/emotion-dataset), dengan sedikit perubahan

# Import Library
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
import tensorflow as tf

"""# Load Data dan membuat dataframe"""

df = pd.read_csv('emotion_text.csv')
df

"""# cek null value"""

df.isna().sum()

"""# CEK DUPLIKAT PADA kolom, dan drop jika ada"""

# Mengecek duplikat
duplicate_rows = df[df.duplicated()]
print(duplicate_rows)

# Jika Anda ingin menghapus duplikat dari DataFrame
df = df.drop_duplicates()

"""# One Hot Encoding dan mengganti nama kolom sesuai nama emosi"""

category = pd.get_dummies(df.label)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='label')

kolom_mapping = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'netral'
}
df_baru = df_baru.rename(columns=kolom_mapping)
df_baru.head()

"""# mengubah kolom jadi array"""

text = df_baru['text'].values
label_columns = ['sadness','joy','love','anger','fear','netral']
label = df_baru[label_columns].values

"""# Split Data"""

text_latih, text_test, label_latih, label_test = train_test_split(text, label, test_size=0.2)

"""# TOKENIZATION, SEQUENCE, PADDING"""

def preprocess_text(text):
    # Menghapus HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Konversi ke huruf kecil
    text = text.lower()

    # Tokenisasi kata
    tokens = word_tokenize(text)

    # Menghapus tanda baca dan karakter khusus
    tokens = [word for word in tokens if word.isalnum()]

    # Menghilangkan stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatisasi
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return tokens

# Membersihkan teks menggunakan fungsi preprocess_text
cleaned_content_latih = [preprocess_text(sentence) for sentence in text_latih]
cleaned_content_test = [preprocess_text(sentence) for sentence in text_test]

# Tokenisasi menggunakan Tokenizer
tokenizer = Tokenizer(num_words=10000, oov_token='-')
tokenizer.fit_on_texts(cleaned_content_latih)

# Konversi teks ke urutan angka menggunakan Tokenizer
sequence_latih = tokenizer.texts_to_sequences(cleaned_content_latih)
sequence_test = tokenizer.texts_to_sequences(cleaned_content_test)

# Padding
latih_padd = pad_sequences(sequence_latih, padding='post', maxlen=20, truncating='post')
test_padd = pad_sequences(sequence_test, padding='post', maxlen=20, truncating='post')

"""# MEMBUAT MODEL LSTM + EMBEDDING"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=128),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(6, activation='softmax')
])

model.summary()

# Optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

"""# TRAIN MODEL"""

# Callback
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
# batch size
batch_size = 64

# Train
epochs = 50
history = model.fit(latih_padd, label_latih, epochs=epochs, batch_size=batch_size,
          validation_data=(test_padd, label_test), verbose=2,
          callbacks=[early_stopping])

# Dapatkan nilai akurasi dari histori pelatihan
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Dapatkan nilai epochs yang sebenarnya (mungkin lebih kecil jika menggunakan EarlyStopping)
actual_epochs = len(train_accuracy)

# Plot akurasi pelatihan dan validasi
epochs_range = range(1, actual_epochs + 1)
plt.plot(epochs_range, train_accuracy, label='Accuracy')
plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')
plt.title('Train and Validation Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Dapatkan nilai akurasi dari histori pelatihan
train_accuracy = history.history['loss']
val_accuracy = history.history['val_loss']

# Dapatkan nilai epochs yang sebenarnya (mungkin lebih kecil jika menggunakan EarlyStopping)
actual_epochs = len(train_accuracy)

# Plot akurasi pelatihan dan validasi
epochs_range = range(1, actual_epochs + 1)
plt.plot(epochs_range, train_accuracy, label='loss')
plt.plot(epochs_range, val_accuracy, label='Validation loss')
plt.title('Train and Validation loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend()
plt.show()